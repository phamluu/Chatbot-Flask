from functools import lru_cache
import pickle
from flask import current_app
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from langdetect import detect
import os
from app.models import Intent, IntentResponse
from app.services.model_holder import model_holder


import torch.nn.functional as F


torch.set_num_threads(1)
# Thiáº¿t bá»‹ sá»­ dá»¥ng
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Biáº¿n toÃ n cá»¥c cho model
MODEL_NAME = "./models/vibert4news_finetuned"
model = None
tokenizer = None
label_encoder = None

@lru_cache(maxsize=512)
def predict_intent_cached(text: str) -> str:
    # Ä‘áº£m báº£o model Ä‘Ã£ load vÃ  Ä‘Æ°á»£c Ä‘Æ°a vá» device, vÃ  á»Ÿ cháº¿ Ä‘á»™ eval
    if model_holder.model is None:
        print("ğŸ” Loading model...")
        model_holder.load()
    model = model_holder.model
    tokenizer = model_holder.tokenizer
    label_encoder = model_holder.label_encoder
    device = model_holder.device

    # Ä‘áº£m báº£o model trÃªn device vÃ  á»Ÿ cháº¿ Ä‘á»™ eval
    model.to(device)
    model.eval()

    # tokenize vÃ  chuyá»ƒn táº¥t cáº£ tensor lÃªn device (an toÃ n hÆ¡n so vá»›i inputs.to(device))
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        # Má»™t sá»‘ model tráº£ tuple, má»™t sá»‘ tráº£ object vá»›i .logits
        logits = outputs.logits if hasattr(outputs, "logits") else outputs[0]
        # print(f"ğŸ§© Logits for '{text}':", logits.cpu().numpy())
        # print("Predicted index:", int(logits.argmax(dim=-1)))

        # kiá»ƒm tra logits Ä‘á»ƒ debug náº¿u cáº§n
        # print("logits:", logits.cpu().numpy())

        # láº¥y xÃ¡c suáº¥t vÃ  lá»›p dá»± Ä‘oÃ¡n
        probs = F.softmax(logits, dim=-1)
        predicted_idx = int(probs.argmax(dim=-1).cpu().item())
        predicted_prob = float(probs.max().cpu().item())
    # tráº£ vá» nhÃ£n gá»‘c qua LabelEncoder
    #print("Label encoder classes:", label_encoder.classes_)

    try:
        label = label_encoder.inverse_transform([predicted_idx])[0]
    except Exception as e:
        print("Lá»—i khi inverse_transform label_encoder:", e)
        # fallback: in ra classes Ä‘á»ƒ debug
        print("label_encoder.classes_:", getattr(label_encoder, "classes_", None))
        raise
    return label

# @lru_cache(maxsize=512)
# def predict_intent_cached(text: str) -> str:
#     if model_holder.model is None:
#         print("ğŸ” Loading model...")
#         model_holder.load()
#     tokenizer = model_holder.tokenizer
#     model = model_holder.model
#     label_encoder = model_holder.label_encoder
#     device = model_holder.device
#     inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
#     with torch.no_grad():
#         logits = model(**inputs).logits
#         predicted_class = logits.argmax(dim=1).item()
#     print(f"Text cá»§a báº¡n: {text}")
#     print(f"Ã Ä‘á»‹nh cá»§a báº¡n: {predicted_class}")
#     return label_encoder.inverse_transform([predicted_class])[0]

conversation_history = []

def generate_local_response(message: str, intent_code: str) -> str:
    from app import db  # âœ… Äáº·t trong hÃ m Ä‘á»ƒ khÃ´ng gÃ¢y lá»—i khi import sá»›m
    with current_app.app_context():
        try:
            intent_reply_map = {
                #ir.intent.intent_code: ir.response_text
                ir.intent.intent_code: (ir.intent.description, ir.response_text)
                for ir in IntentResponse.query.join(Intent).all()
            }
        except Exception as e:
            return "ğŸš« KhÃ´ng thá»ƒ truy xuáº¥t dá»¯ liá»‡u pháº£n há»“i. Vui lÃ²ng kiá»ƒm tra cÆ¡ sá»Ÿ dá»¯ liá»‡u."

    if not intent_reply_map:
        return "ğŸš« KhÃ´ng cÃ³ dá»¯ liá»‡u pháº£n há»“i. Vui lÃ²ng kiá»ƒm tra cÆ¡ sá»Ÿ dá»¯ liá»‡u intents/intent_responses."
    intent_info = intent_reply_map.get(intent_code)
    if intent_info is None:
        reply_text = "Xin lá»—i, tÃ´i chÆ°a hiá»ƒu yÃªu cáº§u cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n khÃ´ng?"
        return f"ğŸ§ {reply_text}"
    intent_description, response_text = intent_info
    conversation_history.append((message, intent_code))
    return f"ğŸ§  {response_text}"

