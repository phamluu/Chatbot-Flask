import pickle
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from langdetect import detect
import os
from app import db
from app.models import Intent, IntentResponse

# âœ… Thiáº¿t láº­p thiáº¿t bá»‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… Biáº¿n toÃ n cá»¥c nhÆ°ng chÆ°a khá»Ÿi táº¡o model/tokenizer
MODEL_NAME = "./models/vibert4news_finetuned"
model = None
tokenizer = None
label_encoder = None

def load_model():
    """Táº£i model, tokenizer, vÃ  label encoder náº¿u chÆ°a load."""
    global model, tokenizer, label_encoder

    if model is None or tokenizer is None or label_encoder is None:
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

        # Load label encoder
        with open(os.path.join(MODEL_NAME, "label_encoder.pkl"), "rb") as f:
            label_encoder = pickle.load(f)

        # Load model
        num_labels = len(label_encoder.classes_)
        model_instance = AutoModelForSequenceClassification.from_pretrained(
            MODEL_NAME,
            num_labels=num_labels,
            ignore_mismatched_sizes=True
        ).to(device)

        model = model_instance

# âœ… PhÃ¡t hiá»‡n ngÃ´n ngá»¯
def detect_language(text):
    try:
        return detect(text)
    except:
        return "unknown"

# âœ… TÃ¡ch tá»« khÃ³a tiáº¿ng Viá»‡t
def extract_keywords(text):
    lang = detect_language(text)
    if lang == "vi":
        from pyvi import ViTokenizer  # âœ… TrÃ¬ hoÃ£n import
        tokens = ViTokenizer.tokenize(text).split()
        return list(set([w for w in tokens if len(w) > 2]))
    return []


# âœ… Dá»± Ä‘oÃ¡n intent tá»« vÄƒn báº£n
def predict_intent(text):
    load_model()  # âœ… Äáº£m báº£o model/tokenizer/encoder Ä‘Æ°á»£c load
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        logits = model(**inputs).logits
        predicted_class = logits.argmax(dim=1).item()
    return label_encoder.inverse_transform([predicted_class])[0]

# âœ… Sinh pháº£n há»“i tá»« intent
conversation_history = []

def generate_local_response(message):
    intent = predict_intent(message)  # Dá»± Ä‘oÃ¡n intent
    #response = f"ğŸ“Œ Ã Ä‘á»‹nh cá»§a báº¡n lÃ : **{intent}**.\n"
    response = f""
    intent_reply_map = {
        intent_response.intent.intent_code: intent_response.response_text
        for intent_response in IntentResponse.query.join(Intent).all()
    }
    if not intent_reply_map:
        return "ğŸš« KhÃ´ng cÃ³ dá»¯ liá»‡u pháº£n há»“i. Vui lÃ²ng kiá»ƒm tra cÆ¡ sá»Ÿ dá»¯ liá»‡u intents/intent_responses."

    reply_text = intent_reply_map.get(
        intent,
        "Xin lá»—i, tÃ´i chÆ°a hiá»ƒu yÃªu cáº§u cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n khÃ´ng?"
    )
    # intent_reply_map = {
    #     "greeting": "Xin chÃ o báº¡n! TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ hÃ´m nay?",
    #     "website": "Báº¡n Ä‘ang quan tÃ¢m Ä‘áº¿n thiáº¿t káº¿ website Ä‘Ãºng khÃ´ng?",
    #     "price": "Báº¡n muá»‘n há»i vá» giÃ¡ cáº£ pháº£i khÃ´ng?",
    #     "delivery": "Báº¡n Ä‘ang há»i vá» giao hÃ ng?",
    #     "contact": "Báº¡n muá»‘n Ä‘Æ°á»£c liÃªn há»‡ láº¡i chá»©?",
    #     "other": "TÃ´i chÆ°a rÃµ yÃªu cáº§u cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ nÃ³i rÃµ hÆ¡n khÃ´ng?"
    # }

    conversation_history.append((message, intent))
    response += reply_text

    return response
